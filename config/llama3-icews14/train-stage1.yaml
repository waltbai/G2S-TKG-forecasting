# Model
model_name_or_path: /home/bingxing2/home/scx6592/zuoyuxin/LLM/LLaMA3-8B
num_predictions: 30
remove_duplicates: false

# Fine-tune
stage: sft
template: default
finetuning_type: lora
lora_target: q_proj,v_proj

# Dataset setting
dataset_dir: /home/bingxing2/home/scx6592/bailong/data/tkg_data
prepare_dir: /home/bingxing2/home/scx6592/bailong/data/llm4tkg/prepare
train_dataset: ICEWS14
valid_dataset: ICEWS14
test_dataset: ICEWS14
history_type: entity
history_direction: uni
history_length: 30
anonymize_strategy: session
anonymize_prefix: false
time_process_strategy: query
vague_time: false
prompt_construct_strategy: inline-rel
candidate_relabel: false
cutoff_len: 2048

# Train
output_dir: /home/bingxing2/home/scx6592/bailong/data/llm4tkg/saves/llama3-icews14-stage1-sft
overwrite_output_dir: true
do_train: true
do_eval: false
do_predict: false
per_device_train_batch_size: 1
gradient_accumulation_steps: 2  # should adapt to parallel training
learning_rate: 0.0001
num_train_epochs: 1.0
lr_scheduler_type: cosine
warmup_steps: 0.1
fp16: true
logging_steps: 10
save_steps: 500
plot_loss: true
